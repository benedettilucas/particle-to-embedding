{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e40789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from superpoint.models.superpoint_pytorch import SuperPoint\n",
    "from accelerated_features.modules.xfeat import XFeat\n",
    "from accelerated_features.modules.lighterglue import LighterGlue\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "xfeat = XFeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6864ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path = \"aerial-image.jpeg\"\n",
    "#img_path = \"UFRGS-01-2017.png\"\n",
    "#weights_path = \"SuperPoint/weights/superpoint_v6_from_tf.pth\"\n",
    "#img = Image.open(img_path).convert(\"L\")\n",
    "#input = {\"image\": transforms.ToTensor()(img).unsqueeze(0)} # from (1, H, W) to (1, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_keypoints(\n",
    "    image_shape,\n",
    "    num_keypoints,\n",
    "    device=\"cpu\",\n",
    "    dtype=torch.float32,\n",
    "    operation=\"random\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera keypoints de acordo com o tipo de operação especificada.\n",
    "\n",
    "    Args:\n",
    "        image_shape: tuple (H, W) ou (B, C, H, W)\n",
    "        num_keypoints: int, número de keypoints a gerar\n",
    "        device: str, 'cpu' ou 'cuda'\n",
    "        dtype: torch.dtype, tipo dos valores\n",
    "        operation: \n",
    "            - \"random\": gera keypoints aleatórios uniformes (default)\n",
    "            - tuple (\"localized\", (x, y), dispersion): gera keypoints próximos a (x, y)\n",
    "              com dispersão gaussiana (desvio padrão = dispersion)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor (B, num_keypoints, 2): coordenadas (x, y)\n",
    "    \"\"\"\n",
    "    # Extrai H e W mesmo que venha de shape 4D\n",
    "    if len(image_shape) == 4:\n",
    "        _, _, H, W = image_shape\n",
    "        B = image_shape[0]\n",
    "    elif len(image_shape) == 2:\n",
    "        H, W = image_shape\n",
    "        B = 1\n",
    "    else:\n",
    "        raise ValueError(\"image_shape deve ser (H, W) ou (B, C, H, W).\")\n",
    "\n",
    "    # Caso 1: operação aleatória uniforme\n",
    "    if operation == \"random\":\n",
    "        xs = torch.rand((B, num_keypoints, 1), device=device, dtype=dtype) * (W - 1)\n",
    "        ys = torch.rand((B, num_keypoints, 1), device=device, dtype=dtype) * (H - 1)\n",
    "\n",
    "    # Caso 2: operação localizada em torno de (x, y)\n",
    "    elif isinstance(operation, tuple) and len(operation) == 3 and operation[0] == \"localized\":\n",
    "        _, center, dispersion = operation\n",
    "        cx, cy = center\n",
    "\n",
    "        # Cria ruído gaussiano ao redor do centro\n",
    "        xs = torch.normal(mean=cx, std=dispersion, size=(B, num_keypoints, 1),\n",
    "                          device=device, dtype=dtype)\n",
    "        ys = torch.normal(mean=cy, std=dispersion, size=(B, num_keypoints, 1),\n",
    "                          device=device, dtype=dtype)\n",
    "\n",
    "        # Clampa coordenadas para ficarem dentro da imagem\n",
    "        xs = xs.clamp(0, W - 1)\n",
    "        ys = ys.clamp(0, H - 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"operation deve ser 'random' ou ('localized', (x, y), dispersion)\"\n",
    "        )\n",
    "\n",
    "    keypoints = torch.cat([xs, ys], dim=-1)\n",
    "    return keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ada198",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_vectors(\n",
    "    [feats0[\"descriptors\"][0], feats00['descriptors'][1880]],\n",
    "    labels=[\"from user\", \"from xfeat\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = feats00['descriptors'][1880]\n",
    "reference = descs[0]\n",
    "sim = cosine_sim(target, reference)\n",
    "sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6373607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_xfeat_at_keypoints(\n",
    "    model,        # instância do XFeat\n",
    "    x,            # (B, C, H, W)\n",
    "    keypoints     # (B, N, 2) em pixels originais\n",
    "):\n",
    "    x, rh1, rw1 = model.preprocess_tensor(x)\n",
    "    B, _, H1, W1 = x.shape\n",
    "\n",
    "    M1, K1, H1_map = model.net(x)\n",
    "    M1 = F.normalize(M1, dim=1)\n",
    "\n",
    "    # pixel original → pixel preprocessado\n",
    "    kpts_resized = keypoints / torch.tensor(\n",
    "        [rw1, rh1], device=keypoints.device\n",
    "    ).view(1, 1, 2)\n",
    "\n",
    "    feats = model.interpolator(\n",
    "        M1,\n",
    "        kpts_resized,\n",
    "        H=H1,\n",
    "        W=W1\n",
    "    )\n",
    "\n",
    "    feats = F.normalize(feats, dim=-1)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32435a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = compute_xfeat_at_keypoints(model=xfeat, x=scene_input[\"image\"], keypoints=p_kpts)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "descs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407969ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ponto detectado pelo xfeat:  {feats00['keypoints'][pos[-1]]}\")\n",
    "print(f\"ponto inserido pelo usuario: {feats0['keypoints'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_final = 100000\n",
    "pos = []\n",
    "for e,k in enumerate(feats00['keypoints']):\n",
    "    dist = k-p_kpts\n",
    "    dist = 0.5*(dist[0][0][0]**2 + dist[0][0][1]**2)\n",
    "    if dist < dist_final:\n",
    "        dist_final = dist\n",
    "        pos.append(e)\n",
    "print(pos[-1], dist_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3182949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropassion(img):\n",
    "    # Carregar e converter para tons de cinza\n",
    "    reference_img = Image.open(img).convert(\"L\")\n",
    "\n",
    "    # Converter para numpy\n",
    "    img_np = np.array(reference_img)\n",
    "\n",
    "    # Altura atual e nova altura\n",
    "    h, w = img_np.shape\n",
    "    new_h = 421\n",
    "\n",
    "    # Largura proporcional\n",
    "    scale = new_h / h\n",
    "    new_w = int(w * scale)\n",
    "\n",
    "    # Resize proporcional\n",
    "    resized_img = reference_img.resize((new_w, new_h), Image.BICUBIC)\n",
    "    resized_np = np.array(resized_img)\n",
    "\n",
    "    # ---- CROP CENTRAL PARA 421 x 421 ----\n",
    "    target_size = 421\n",
    "\n",
    "    # calcular início e fim do corte horizontal\n",
    "    excess = new_w - target_size\n",
    "    left = excess // 2\n",
    "    right = left + target_size\n",
    "\n",
    "    crop_np = resized_np[:, left:right]\n",
    "    return crop_np\n",
    "\n",
    "scene_img = Image.open('UFRGS-01-2017.png').convert(\"L\")\n",
    "scene_input = {\"image\": transforms.ToTensor()(scene_img).unsqueeze(0)} \n",
    "\n",
    "reference_img = Image.open('009-align.jpg').convert(\"L\")\n",
    "reference_input = {\"image\": transforms.ToTensor()(reference_img).unsqueeze(0)}\n",
    "#crop_np = cropassion('009-align.jpg')\n",
    "#reference_input = {\"image\": transforms.ToTensor()(crop_np).unsqueeze(0)} \n",
    "\n",
    "particles_kpts = generate_random_keypoints((4800, 4800), num_keypoints=1, operation=(\"localized\", (2860, 1700), 1000), device=\"cuda\") #operation='random'\n",
    "gt_kpt = torch.tensor([[[2860, 1700]]], device='cuda:0')\n",
    "particles_kpts = torch.cat((gt_kpt, particles_kpts), dim=1)\n",
    "p_kpts = torch.tensor([[[1272, 1715]]], device='cuda:0')\n",
    "print(\"Mapa da missão:\")\n",
    "feats0 = xfeat.computeAtKeypoints2(scene_input[\"image\"], p_kpts)[0] # particles_kpts)[0]\n",
    "feats00= xfeat.detectAndCompute(scene_input[\"image\"], top_k = 2000)[0]\n",
    "print(\"Imagem da nadir redimensionada at keypoints:\")\n",
    "feats1 = xfeat.detectAndCompute(reference_input[\"image\"], top_k = 1)[0]\n",
    "feats1['keypoints'] = gt_kpt[0] #1275 1722\n",
    "#feats1['descriptors'] = feats1['descriptors'].repeat(particles_kpts.shape[1], 1)\n",
    "print(\"Imagem da nadir redimensionada global\")\n",
    "desc = xfeat.computeGlobalDescriptor(reference_input[\"image\"], resize_to_receptive=False)[0]\n",
    "desc['descriptors'] = desc['descriptors'].repeat(particles_kpts.shape[1], 1)\n",
    "desc['keypoints'] = particles_kpts[0]\n",
    "\n",
    "feats0.update({'image_size': (scene_input['image'][0][0].shape[1], scene_input['image'][0][0].shape[0])})\n",
    "feats1.update({'image_size': (scene_input['image'][0][0].shape[1], scene_input['image'][0][0].shape[0])})\n",
    "desc.update({'image_size': (scene_input['image'][0][0].shape[1], scene_input['image'][0][0].shape[0])})\n",
    "\n",
    "# VERSOES CROPADAS\n",
    "#d33 = xfeat.detectAndCompute(transforms.ToTensor()(crop).unsqueeze(0), top_k = 1)[0]\n",
    "#d44 = xfeat.computeGlobalDescriptor(transforms.ToTensor()(crop).unsqueeze(0), resize_to_receptive=False)[0]\n",
    "mkpts_0, mkpts_1, _, output = xfeat.match_lighterglue(feats0, desc)\n",
    "\n",
    "print(mkpts_0, mkpts_1)\n",
    "\n",
    "keypoints = particles_kpts[0]\n",
    "keypoints = torch.cat((feats0['keypoints'][0].unsqueeze(0), feats00['keypoints'][1880].unsqueeze(0)), dim=0)\n",
    "#p_kpts = torch.tensor([[[1272, 1715]]], device='cuda:0')\n",
    "if keypoints is not None:\n",
    "    keypoints = keypoints.detach().cpu().numpy()  # [N,2]\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(scene_img, cmap=\"gray\")\n",
    "    plt.scatter(keypoints[:,0], keypoints[:,1], c='r', s=10)  # x,y\n",
    "    plt.title(\"Keypoints detectados pelo SuperPoint\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"A saída não contém 'keypoints'\")\n",
    "\n",
    "#keypoints = keypoints_desc\n",
    "keypoints = None\n",
    "if keypoints is not None:\n",
    "    keypoints = keypoints.detach().cpu().numpy()  # [N,2]\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(reference_img, cmap=\"gray\")\n",
    "    plt.scatter(keypoints[:,0], keypoints[:,1], c='r', s=10)  # x,y\n",
    "    plt.title(\"Keypoints detectados pelo SuperPoint\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"A saída não contém 'keypoints'\")\n",
    "\n",
    "# Plot para visualização\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(crop_np, cmap=\"gray\")\n",
    "plt.title(f\"Crop final: {crop_np.shape[1]}x{crop_np.shape[0]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_n_vectors(vectors, labels=None, title=\"Plot de múltiplos vetores 1×64\"):\n",
    "    \"\"\"\n",
    "    Plota N vetores PyTorch de shape (1, 64) no mesmo gráfico.\n",
    "\n",
    "    Args:\n",
    "        vectors (list): lista de tensores PyTorch, cada um com shape (1, 64)\n",
    "        labels (list): lista opcional de rótulos. Se None, gera rótulos automáticos.\n",
    "        title (str): título do gráfico\n",
    "    \"\"\"\n",
    "    # Número de vetores\n",
    "    n = len(vectors)\n",
    "\n",
    "    # Rotulagem automática caso labels não seja fornecido\n",
    "    if labels is None:\n",
    "        labels = [f\"vetor_{i}\" for i in range(n)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for i, v in enumerate(vectors):\n",
    "        # converter para numpy e remover dim extra\n",
    "        v_np = v.detach().cpu().numpy().squeeze()  # vira (64,)\n",
    "\n",
    "        plt.plot(v_np, label=labels[i], alpha=0.8)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimensão (0–63)\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_n_vectors(\n",
    "#    [feats0[\"descriptors\"][0], feats0[\"descriptors\"][1], feats1[\"descriptors\"]],\n",
    "#    labels=[\"particle_wrong\", \"particle_true\", \"uav\"]\n",
    "#)\n",
    "\n",
    "def cosine_sim(feat0, feat1):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade do cosseno entre dois vetores (1, D).\n",
    "\n",
    "    Args:\n",
    "        feat0 (torch.Tensor): tensor shape (1, D)\n",
    "        feat1 (torch.Tensor): tensor shape (1, D)\n",
    "\n",
    "    Returns:\n",
    "        float: similaridade do cosseno\n",
    "    \"\"\"\n",
    "    # remover dimensões extras -> (D,)\n",
    "    v0 = feat0.squeeze()\n",
    "    v1 = feat1.squeeze()\n",
    "\n",
    "    # similaridade do cosseno\n",
    "    cos = F.cosine_similarity(v0, v1, dim=0)\n",
    "\n",
    "    return cos.item()\n",
    "\n",
    "\"\"\"\n",
    "d0 = feats0[\"descriptors\"][1]   # shape (1, 64)\n",
    "d1 = feats0[\"descriptors\"][0]\n",
    "d2 = feats1[\"descriptors\"]   # shape (1, 64)\n",
    "d3 = d33[\"descriptors\"]\n",
    "d4 = d44[\"descriptors\"]\n",
    "d5 = desc[\"descriptors\"][0]\n",
    "\n",
    "sim1 = cosine_sim(d0, d2)\n",
    "sim2 = cosine_sim(d1, d2)\n",
    "sim3 = cosine_sim(d3, d2)\n",
    "sim4 = cosine_sim(d4, d2)\n",
    "sim8 = cosine_sim(d3, d5)\n",
    "sim9 = cosine_sim(d4, d5)\n",
    "sim10 = cosine_sim(d1, d5)\n",
    "sim11 = cosine_sim(d0, d5)\n",
    "\n",
    "print(\"Similaridade usando D&C para camera:\")\n",
    "print(\"com a particula errada: \", sim1)\n",
    "print(\"com particula certa:    \", sim2)\n",
    "print(\"com cropada D&C:        \", sim3)\n",
    "print(\"com cropada CG:         \", sim4)\n",
    "\n",
    "print(\"\\nSimilaridade usando CG para camera:\")\n",
    "print(\"com a particula errada: \", sim11)\n",
    "print(\"com particula certa:    \", sim10)\n",
    "print(\"com cropada D&C:        \", sim8)\n",
    "print(\"com cropada CG:         \", sim9)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_centered(image, center, patch_size):\n",
    "    \"\"\"\n",
    "    image: (C, H, W)\n",
    "    center: (x, y)\n",
    "    patch_size: int\n",
    "    \"\"\"\n",
    "    _, C, H, W = image.shape\n",
    "    x, y = center\n",
    "\n",
    "    half = patch_size // 2\n",
    "\n",
    "    x1 = int(x - half)\n",
    "    y1 = int(y - half)\n",
    "    x2 = int(x + half)\n",
    "    y2 = int(y + half)\n",
    "\n",
    "    # limites reais da imagem\n",
    "    ix1 = max(0, x1)\n",
    "    iy1 = max(0, y1)\n",
    "    ix2 = min(W, x2)\n",
    "    iy2 = min(H, y2)\n",
    "    crop = image[:, :, iy1:iy2, ix1:ix2]\n",
    "\n",
    "    # padding se saiu da imagem\n",
    "    pad_left   = ix1 - x1\n",
    "    pad_top    = iy1 - y1\n",
    "    pad_right  = x2 - ix2\n",
    "    pad_bottom = y2 - iy2\n",
    "\n",
    "    crop = F.pad(\n",
    "        crop,\n",
    "        (pad_left, pad_right, pad_top, pad_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0.0\n",
    "    )\n",
    "\n",
    "    return crop  # (C, patch_size, patch_size)\n",
    "\n",
    "def pad_to_square(tensor):\n",
    "    \"\"\"\n",
    "    tensor: (C, H, W)\n",
    "    \"\"\"\n",
    "    _, C, H, W = tensor.shape\n",
    "    size = max(H, W)\n",
    "\n",
    "    pad_h = size - H\n",
    "    pad_w = size - W\n",
    "\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "\n",
    "    return F.pad(\n",
    "        tensor,\n",
    "        (pad_left, pad_right, pad_top, pad_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0.0\n",
    "    )\n",
    "\n",
    "def resize_patch(patch, target_size=416):\n",
    "    \"\"\"\n",
    "    patch: (C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    patch = F.interpolate(\n",
    "        patch,\n",
    "        size=(target_size, target_size),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    return patch#.squeeze(0)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "\n",
    "def evaluate_patch_scales(\n",
    "    image,\n",
    "    keypoint,\n",
    "    D_ref,\n",
    "    model,\n",
    "    patch_sizes=(64, 96, 128, 160, 192, 224, 256),\n",
    "    input_size=416\n",
    "):\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "        dict {patch_size: similarity}\n",
    "    \"\"\"\n",
    "    sims = {}\n",
    "\n",
    "    for ps in patch_sizes:\n",
    "        crop = crop_centered(image, keypoint, ps)\n",
    "        crop = pad_to_square(crop)\n",
    "        crop = resize_patch(crop, input_size)\n",
    "\n",
    "        D_patch = model.computeGlobalDescriptor(crop, resize_to_receptive=False)[0]\n",
    "\n",
    "\n",
    "        if D_patch['descriptors'].ndim == 2:\n",
    "            D_patch = D_patch['descriptors'].squeeze(0)\n",
    "\n",
    "        sims[ps] = cosine_similarity(D_patch, D_ref)\n",
    "        print(f\"patch: {ps} - sim: {sims[ps]}\")\n",
    "\n",
    "    return sims\n",
    "\n",
    "def best_patch_size(similarities):\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "D_ref = feats1[\"descriptors\"][0] # from local\n",
    "x, y = feats0['keypoints'][1]\n",
    "\n",
    "sims = evaluate_patch_scales(\n",
    "    image=scene_input['image'],\n",
    "    keypoint=(x, y),\n",
    "    D_ref=D_ref,\n",
    "    model=xfeat\n",
    ")\n",
    "\n",
    "best_size = best_patch_size(sims)\n",
    "print()\n",
    "\n",
    "D_ref = desc[\"descriptors\"][0] # from global\n",
    "x, y = feats0['keypoints'][0]\n",
    "\n",
    "sims = evaluate_patch_scales(\n",
    "    image=scene_input['image'],\n",
    "    keypoint=(x, y),\n",
    "    D_ref=D_ref,\n",
    "    model=xfeat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_receptive_field(img, keypoint, rf_size=421, stride=32, color='lime', linewidth=2):\n",
    "    \"\"\"\n",
    "    Plota o campo receptivo aproximado do XFeat centrado em um keypoint.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): imagem (H, W, 3) ou (H, W)\n",
    "        keypoint (tuple): (x, y) na imagem original\n",
    "        rf_size (int): tamanho do campo receptivo (em pixels)\n",
    "        stride (int): stride acumulado da rede\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Converte tensor para numpy e remove dimensões extras\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu().numpy()\n",
    "    if img.ndim == 4:  # (B, C, H, W)\n",
    "        img = img[0]   # remove batch\n",
    "    if img.ndim == 3 and img.shape[0] == 1:  # (1, H, W) -> (H, W)\n",
    "        img = img[0]\n",
    "    elif img.ndim == 3 and img.shape[0] == 3:  # (3, H, W) -> (H, W, 3)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "    cx, cy = keypoint\n",
    "\n",
    "    # Define bordas do campo receptivo\n",
    "    half_rf = rf_size // 2\n",
    "    x0 = max(cx - half_rf, 0)\n",
    "    y0 = max(cy - half_rf, 0)\n",
    "    x1 = min(cx + half_rf, W)\n",
    "    y1 = min(cy + half_rf, H)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "\n",
    "    # Retângulo do campo receptivo\n",
    "    rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0,\n",
    "                             linewidth=linewidth, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Keypoint central\n",
    "    ax.scatter(cx, cy, s=50, c='red', marker='x', label='Centro (keypoint)')\n",
    "\n",
    "    ax.set_title(f\"Campo Receptivo ({rf_size} px) centrado em ({cx}, {cy})\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_receptive_field2(img, keypoint, rf_size=421, stride=32, \n",
    "                         color='lime', linewidth=2):\n",
    "    \"\"\"\n",
    "    Plota o campo receptivo aproximado do XFeat centrado em um keypoint,\n",
    "    e retorna a imagem recortada correspondente ao RF.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray or torch.Tensor): imagem (H, W, 3) ou (H, W)\n",
    "        keypoint (tuple): (x, y) na imagem original\n",
    "        rf_size (int): tamanho do campo receptivo (em pixels)\n",
    "        stride (int): stride acumulado da rede\n",
    "\n",
    "    Returns:\n",
    "        crop_img (np.ndarray): imagem recortada do campo receptivo (Hc, Wc[, C])\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------- Normalização da imagem -----------\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu().numpy()\n",
    "\n",
    "    if img.ndim == 4:        # (B, C, H, W)\n",
    "        img = img[0]\n",
    "    if img.ndim == 3 and img.shape[0] in (1, 3):  # (C, H, W)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "    cx, cy = keypoint\n",
    "\n",
    "    # ----------- Coordenadas do RF -----------\n",
    "    half_rf = rf_size // 2\n",
    "    x0 = max(cx - half_rf, 0)\n",
    "    y0 = max(cy - half_rf, 0)\n",
    "    x1 = min(cx + half_rf, W)\n",
    "    y1 = min(cy + half_rf, H)\n",
    "\n",
    "    # ----------- Crop da imagem -----------\n",
    "    crop_img = img[y0:y1, x0:x1].copy()\n",
    "\n",
    "    # ----------- Plot -----------\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "\n",
    "    rect = patches.Rectangle(\n",
    "        (x0, y0),\n",
    "        x1 - x0,\n",
    "        y1 - y0,\n",
    "        linewidth=linewidth,\n",
    "        edgecolor=color,\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.scatter(cx, cy, s=50, c='red', marker='x', label='Centro (keypoint)')\n",
    "    ax.set_title(f\"Campo Receptivo ({rf_size} px) centrado em ({cx}, {cy})\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_receptive_field3(img, keypoint, rf_size=421, stride=32, \n",
    "                          color='lime', linewidth=2):\n",
    "    \"\"\"\n",
    "    Plota o campo receptivo e retorna o crop do RF.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------- Normalização da imagem -----------\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu().numpy()\n",
    "\n",
    "    # Se (B, C, H, W)\n",
    "    if img.ndim == 4:\n",
    "        img = img[0]\n",
    "\n",
    "    # Se (C, H, W)\n",
    "    if img.ndim == 3 and img.shape[0] in (1, 3):\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # ----------- Coordenadas do keypoint -----------\n",
    "    cx, cy = keypoint\n",
    "    cx = int(cx)\n",
    "    cy = int(cy)\n",
    "\n",
    "    half_rf = rf_size // 2\n",
    "\n",
    "    # ----------- Sempre converter para int -----------\n",
    "    x0 = int(max(cx - half_rf, 0))\n",
    "    y0 = int(max(cy - half_rf, 0))\n",
    "    x1 = int(min(cx + half_rf, W))\n",
    "    y1 = int(min(cy + half_rf, H))\n",
    "\n",
    "    # ----------- Crop -----------\n",
    "    crop_img = img[y0:y1, x0:x1].copy()\n",
    "\n",
    "    # ----------- Plot -----------\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "\n",
    "    rect = patches.Rectangle(\n",
    "        (x0, y0),\n",
    "        x1 - x0,\n",
    "        y1 - y0,\n",
    "        linewidth=linewidth,\n",
    "        edgecolor=color,\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.scatter(cx, cy, s=50, c='red', marker='x')\n",
    "    ax.set_title(f\"RF {rf_size}px centrado em ({cx}, {cy})\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return crop_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "keypoint = feats0['keypoints'][0].cpu().numpy()  # centro da imagem 1280x704\n",
    "print(keypoint)\n",
    "nadir = xfeat.x_receptive\n",
    "#particle_img = plot_receptive_field(scene_input['image'], keypoint, rf_size=421, stride=32)\n",
    "\n",
    "init_t = time.time()\n",
    "c = 0\n",
    "#for kp in feats0['keypoints']:\n",
    "#    crop = plot_receptive_field3(scene_input['image'], kp, rf_size=421)\n",
    "#    c= c+1\n",
    "crop = plot_receptive_field3(scene_input['image'], keypoint, rf_size=421)\n",
    "end_t = time.time()\n",
    "print(c)\n",
    "all_time = (end_t-init_t)\n",
    "print(\"time: \", all_time)\n",
    "#print(f\"avg_time: {(all_time/c)*10**3} ms\")\n",
    "plt.imshow(crop, cmap='gray')\n",
    "plt.title(\"Crop retornado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats0['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2853 1681\n",
    "keypoint = desc['keypoints'][0].cpu().numpy()  # centro da imagem 1280x704\n",
    "\n",
    "nadir = xfeat.x_receptive\n",
    "plot_receptive_field(nadir, keypoint, rf_size=421, stride=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = F.cosine_similarity(desc['descriptors'], feats0['descriptors'], dim=-1)\n",
    "for e,s in enumerate(similarities):\n",
    "    print(e,s,feats0['keypoints'][e].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_receptive_field(model, input_size):\n",
    "    \"\"\"\n",
    "    Calcula o campo receptivo efetivo de cada camada Conv2d da rede.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): modelo PyTorch\n",
    "        input_size (tuple): (B, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts com nome, RF, stride acumulado e tamanho de saída\n",
    "    \"\"\"\n",
    "    rf = 1      # campo receptivo inicial\n",
    "    j = 1       # jump (passo)\n",
    "    start = 0   # offset (pode ser usado pra coordenadas absolutas)\n",
    "    \n",
    "    info = []\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(module, inp, out):\n",
    "        nonlocal rf, j, start\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            k = module.kernel_size[0]\n",
    "            s = module.stride[0]\n",
    "            p = module.padding[0]\n",
    "            d = module.dilation[0]\n",
    "            \n",
    "            rf = rf + ( (k - 1) * d ) * j\n",
    "            start = start + ((k - 1)/2 - p) * j\n",
    "            j = j * s\n",
    "\n",
    "            info.append({\n",
    "                \"layer\": module.__class__.__name__,\n",
    "                \"kernel\": k,\n",
    "                \"stride\": s,\n",
    "                \"padding\": p,\n",
    "                \"rf\": rf,\n",
    "                \"jump\": j,\n",
    "                \"output_shape\": tuple(out.shape)\n",
    "            })\n",
    "\n",
    "    # registrar hooks\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            hooks.append(m.register_forward_hook(hook_fn))\n",
    "    \n",
    "    x = torch.zeros(input_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xfeat.detectAndCompute(x, top_k = 1)\n",
    "    \n",
    "    # remover hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64519638",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info = compute_receptive_field(xfeat, input_size=transforms.ToTensor()(reference_img).unsqueeze(0).shape) \n",
    "rf_info = compute_receptive_field(xfeat, input_size=transforms.ToTensor()(scene_img).unsqueeze(0).shape)\n",
    "for i, layer in enumerate(rf_info):\n",
    "    print(f\"{i:02d} | {layer['layer']:<10} | RF={layer['rf']:>3} | Jump={layer['jump']:>2} | Out={layer['output_shape']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_np = np.array(hist)\n",
    "#print(f\"extract 50.0000 particles points view using xfeat: {hist_np.mean():.3f} ± {hist_np.std():.4f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extractors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
